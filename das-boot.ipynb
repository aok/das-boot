{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_price=500000\n",
    "debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pickle\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pycountry\n",
    "import functools\n",
    "\n",
    "import requests_cache\n",
    "requests_cache.install_cache(expire_after=3600)\n",
    "requests_cache.core.remove_expired_responses()\n",
    "\n",
    "Path('out').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def memoize(f):\n",
    "    memo = {}\n",
    "    def helper(x):\n",
    "        if x not in memo:            \n",
    "            memo[x] = f(x)\n",
    "        return memo[x]\n",
    "    return helper\n",
    "\n",
    "def none_on_error(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            if debug: print(func.__name__+' FAILED with '+str(e))\n",
    "            return None\n",
    "    return wrapper\n",
    "\n",
    "req_headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15'\n",
    "}\n",
    "def get_page(url):\n",
    "    r = requests.get(url, headers=req_headers, allow_redirects=True)\n",
    "    return r.text\n",
    "\n",
    "def get_soup(url, parser='html.parser'):\n",
    "    return BeautifulSoup(\n",
    "        get_page(url),\n",
    "        parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nettivene.com\n",
    "\n",
    "@none_on_error\n",
    "def nv_year(div):\n",
    "    sub_div = div.find('div',class_='vehicle_other_info')\n",
    "    return int(sub_div.ul.li.text)\n",
    "\n",
    "@none_on_error\n",
    "def nv_country_city(div):\n",
    "    sub_div = div.find('div',class_='location_info')\n",
    "    s = sub_div.b.text.split()[0].strip()\n",
    "    if s in [c.name for c in pycountry.countries]: return s, None\n",
    "    return 'Finland', s\n",
    "\n",
    "@none_on_error\n",
    "def nv_price(div):\n",
    "    sub_div = div.find('div',class_='main_price')\n",
    "    return int(sub_div.text.replace(' ','').replace('€',''))\n",
    "\n",
    "\n",
    "def nv_parse_list_page(make,soup):\n",
    "    divs = soup.find_all('div', class_='listingVifUrl')\n",
    "    l = []\n",
    "    for div in divs:\n",
    "        country, city = nv_country_city(div)\n",
    "        l.append(\n",
    "            (\n",
    "                div.div.a['href'],\n",
    "                div.div.a.text.replace(make,''),\n",
    "                nv_year(div),\n",
    "                country,\n",
    "                city,\n",
    "                nv_price(div)\n",
    "\n",
    "            )\n",
    "        )\n",
    "    return l\n",
    "\n",
    "@none_on_error\n",
    "def nv_next_page_url(soup):\n",
    "    return soup.find(\n",
    "            'a',\n",
    "            class_='pageNavigation next_link'\n",
    "        )['href']\n",
    "\n",
    "\n",
    "def nv_listings(make):\n",
    "    next_url = 'https://www.nettivene.com/en/purjevene/'+make.replace(' ','-').lower()\n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        msg = soup.find('div', id='msg')\n",
    "        if msg:\n",
    "            print(msg.text)\n",
    "            return l        \n",
    "        l += nv_parse_list_page(\n",
    "            make,\n",
    "            soup\n",
    "        )\n",
    "        next_url = nv_next_page_url(soup)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#yachtworld\n",
    "\n",
    "@none_on_error\n",
    "def yw_redux_state_json(soup):\n",
    "    script_tag = soup.find('script',string=re.compile('__REDUX_STATE__')).contents[0]\n",
    "    json_str = script_tag[script_tag.index('window.__REDUX_STATE__ = ')+25:script_tag.rfind('}')+1]\n",
    "    return json.loads(json_str)    \n",
    "\n",
    "@none_on_error\n",
    "def yw_price(record):\n",
    "    return record['price']['type']['amount']['EUR']\n",
    "\n",
    "def yw_country(record):\n",
    "    cc = record['location']['countryCode']\n",
    "    country = pycountry.countries.get(alpha_2=cc)\n",
    "    if country: return country.name\n",
    "    return cc\n",
    "\n",
    "@none_on_error\n",
    "def yw_parse_record(r):\n",
    "    return (\n",
    "        r['mappedURL'],\n",
    "        r['model'],\n",
    "        r['year'],\n",
    "#        r['boat']['specifications']['dimensions']['lengths']['nominal']['m'],\n",
    "        yw_country(r),\n",
    "        str(r['location']['city']),\n",
    "        yw_price(r)\n",
    "    )\n",
    "\n",
    "def yw_collect_listings(js):\n",
    "    records = js['search']['searchResults']['search']['records']\n",
    "    return [yw_parse_record(r) for r in records]\n",
    "\n",
    "def yw_has_next(js):\n",
    "    curr_page = int(js['search']['searchResults']['search']['currentPage'])\n",
    "    last_page = int(js['search']['searchResults']['search']['lastPage'])\n",
    "    return (curr_page<last_page)    \n",
    "\n",
    "\n",
    "def yw_listings(make):\n",
    "    url_template='https://www.yachtworld.com/boats-for-sale/condition-used/type-sail/make-{}/?currency=EUR&price=0-{}'\n",
    "    base_url = url_template.format(make.lower().replace(' ','-'),max_price)\n",
    "    url = base_url\n",
    "\n",
    "    l = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        js = yw_redux_state_json(get_soup(url))\n",
    "        if js:\n",
    "            l += yw_collect_listings(js)\n",
    "        \n",
    "            if yw_has_next(js):\n",
    "                page += 1\n",
    "                url = base_url+'&page='+str(page)\n",
    "            else: break\n",
    "        else: break\n",
    "        \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#boat24\n",
    "\n",
    "re_year = re.compile(r'Year Built (\\d\\d\\d\\d)')\n",
    "\n",
    "@none_on_error\n",
    "def b24_year(div):\n",
    "    return int(re_year.search(div.find('ul', {'class':'blurb__description'}).text).group(1))\n",
    "\n",
    "re_price = re.compile(r'EUR (\\d\\d*).(\\d\\d\\d)')\n",
    "\n",
    "@none_on_error\n",
    "def b24_price(div):\n",
    "    g = re_price.search(div.find('p', {'class':'blurb__price'}).text).groups()\n",
    "    return int(g[0]+g[1])\n",
    "\n",
    "def b24_scrape(make,soup):    \n",
    "    \n",
    "    divs = soup.find_all('div', class_='blurb')\n",
    "\n",
    "    l = []\n",
    "    for div in divs:\n",
    "        loc_a = div.find('p', {'class':'blurb__location'}).text.split('»')\n",
    "        city = None,\n",
    "        if len(loc_a) > 1: city = loc_a[1].strip()\n",
    "        l.append(\n",
    "            (\n",
    "                div['data-link'],\n",
    "                div['title'].replace(make,''),\n",
    "                b24_year(div),\n",
    "                loc_a[0].strip(),\n",
    "                city,\n",
    "                b24_price(div),\n",
    "            )\n",
    "        )\n",
    "    return l\n",
    "    \n",
    "@none_on_error\n",
    "def b24_next_url(soup):\n",
    "    return soup.find('a', class_='pagination__next')['href']\n",
    "\n",
    "def b24_listings(make):\n",
    "    next_url = 'https://www.boat24.com/en/sailboats/?src={}&mode=AND&whr=EUR&prs_min=&prs_max={}'.format(\n",
    "        make.replace(' ','+'),\n",
    "        max_price\n",
    "    )\n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        l += b24_scrape(make,soup)\n",
    "        next_url = b24_next_url(soup)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#yachtmarket\n",
    "\n",
    "re_tym_year = re.compile(r'(\\d\\d\\d\\d)\\D')\n",
    "\n",
    "@none_on_error\n",
    "def tym_year(ov):\n",
    "    return int(re_tym_year.search(ov.text).group(1))\n",
    "\n",
    "\n",
    "@none_on_error\n",
    "def tym_price(div):\n",
    "    return int(div.find('div',class_='pricing').span.text.replace('€','').replace('EUR','').replace(',',''))\n",
    "\n",
    "\n",
    "def tym_scrape(make, soup):\n",
    "\n",
    "    divs = soup.find_all('div',class_='result')\n",
    "\n",
    "    l = []\n",
    "    for div in divs:\n",
    "        a = div.find('a', class_='boat-name')\n",
    "        ov = div.find('div', class_='overview')\n",
    "        loc_a = div.find('div', class_='location').text.split(',')\n",
    "        l.append(\n",
    "            (\n",
    "                'https://www.theyachtmarket.com'+a['href'].split('?')[0],\n",
    "                a.text.replace(make,'').strip(),\n",
    "                tym_year(ov),\n",
    "                loc_a[-1],\n",
    "                loc_a[0],\n",
    "                tym_price(div)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    return l\n",
    "\n",
    "\n",
    "@none_on_error\n",
    "def tym_next_url(soup):\n",
    "    return 'https://www.theyachtmarket.com/en/boats-for-sale/search/'+soup.find('a', rel='next')['href']\n",
    "\n",
    "def tym_listings(make):\n",
    "    next_url = 'https://www.theyachtmarket.com/en/boats-for-sale/search/?manufacturermodel={}&currency=eur&lengthunit=metres&showsail=1'.format(\n",
    "            make.replace(' ','+').lower()\n",
    "        )\n",
    "    \n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        l += tym_scrape(make,soup)\n",
    "        next_url = tym_next_url(soup)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scanboat.com\n",
    "@none_on_error\n",
    "def sb_price(s):\n",
    "    return int(s.p.text.replace('EUR','').replace(',',''))\n",
    "\n",
    "re_sb_year = re.compile(r'Year : (\\d\\d\\d\\d)')\n",
    "re_sb_country= re.compile(r'Country : (.*)')\n",
    "\n",
    "def sb_scrape(make, soup):\n",
    "    \n",
    "    divs = soup.find_all('div', class_='item')\n",
    "\n",
    "    l = []\n",
    "    for div in divs:\n",
    "        header = div.find('header',class_='item__header')\n",
    "        body = div.find('section',class_='item__body')\n",
    "        url = 'https://www.scanboat.com'+div.a['href']\n",
    "        price_tags = header.find_all('p')\n",
    "\n",
    "        if price_tags:\n",
    "            l.append(\n",
    "                (\n",
    "                    url,\n",
    "                    header.section.text.replace(make,'').replace(' - ',''),\n",
    "                    int(re_sb_year.search(body.p.text).group(1)),\n",
    "                    re_sb_country.search(body.p.text).group(1),\n",
    "                    None,\n",
    "                    int(price_tags[-1].text.replace('EUR','').replace(',','')),\n",
    "                )\n",
    "            )\n",
    "    return l    \n",
    "\n",
    "@none_on_error\n",
    "def sb_next_url(soup):\n",
    "    return 'https://www.scanboat.com'+soup.find('a',string='Next')['href']\n",
    "\n",
    "    \n",
    "def sb_listings(make):\n",
    "    next_url = 'https://www.scanboat.com/en/boats?SearchCriteria.BoatModelText={}&SearchCriteria.BoatTypeID=1&SearchCriteria.Searched=true&SearchCriteria.ExtendedSearch=False'.format(make.replace(' ','+').lower())\n",
    "    \n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        l += sb_scrape(make,soup)\n",
    "        next_url = sb_next_url(soup)\n",
    "    return l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boats.com\n",
    "re_bcom_price = re.compile(r'€(\\d{1,3})\\,(\\d\\d\\d)')\n",
    "\n",
    "@none_on_error\n",
    "def bcom_price(div):\n",
    "    match = re_bcom_price.search(div.find('div',class_='price').text)\n",
    "    return int(match.group(1)+match.group(2))\n",
    "\n",
    "@none_on_error\n",
    "def bcom_country(loc_a):\n",
    "    s = loc_a[-1]\n",
    "    if s in [c.name for c in pycountry.countries]: return s\n",
    "    return 'United States'\n",
    "\n",
    "@none_on_error\n",
    "def bcom_city(loc_a):\n",
    "    return loc_a[0]\n",
    "\n",
    "\n",
    "def bcom_scrape(make, soup):\n",
    "    list_items = soup.find_all('li', {'data-listing-id': True})\n",
    "    l = list()\n",
    "    for li in list_items:\n",
    "        a = li.div.a\n",
    "        details = li.find('div', class_='details')\n",
    "        loc_a = li.find('div',class_='country').text.split(',')\n",
    "\n",
    "        l.append(\n",
    "            (\n",
    "                'https://www.boats.com'+a['href'],\n",
    "                details.div.h2.text.replace(make,'').strip(),\n",
    "                int(details.find('div',class_='year').text),\n",
    "                bcom_country(loc_a),\n",
    "                bcom_city(loc_a),\n",
    "                bcom_price(details)\n",
    "            )\n",
    "        )\n",
    "    return l\n",
    "\n",
    "@none_on_error\n",
    "def bcom_next_url(soup):\n",
    "    return 'https://www.boats.com'+soup.find_all('a',class_='next')[-1]['href']\n",
    "\n",
    "def bcom_listings(make):\n",
    "    url_template = 'https://www.boats.com/boats-for-sale/?boat-type=sail&make={}&price-to={}&currency=eur'\n",
    "    next_url = url_template.format(\n",
    "        make.lower().replace(' ','-'),\n",
    "        max_price\n",
    "    )\n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url,parser='lxml')\n",
    "        l += bcom_scrape(make,soup)\n",
    "        next_url = bcom_next_url(soup)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_euro_price = re.compile(r'€\\s(\\d{1,3}),(\\d\\d\\d)')\n",
    "@none_on_error\n",
    "def ya_price(div):\n",
    "    span = div.find('span', string=re.compile('price.*'))\n",
    "    m = re_euro_price.search(str(span.parent.contents))\n",
    "    return int(m.group(1)+m.group(2))\n",
    "\n",
    "@none_on_error\n",
    "def ya_city(loc_span):\n",
    "    return loc_span.next_sibling.next_sibling.text[1:]\n",
    "\n",
    "def ya_scrape(make,soup):\n",
    "    divs = soup.find_all('div', class_='boatlist-subbox')\n",
    "\n",
    "    l = list()\n",
    "    for div in divs:\n",
    "        yard_str = div.find('h3').next_sibling.next_sibling.span.text\n",
    "        if make in yard_str:\n",
    "            a = div.find('a',class_='js-hrefBoat')\n",
    "            price_span = div.find('span', string=re.compile('price.*'))\n",
    "            loc_span = div.find('span', string=re.compile('location.*'))\n",
    "            l.append(\n",
    "                (\n",
    "                    'https://www.yachtall.com'+a['href'],\n",
    "                    a.text.replace(make,''),\n",
    "                    int(div.find('b',string=re.compile('\\d\\d\\d\\d')).text),\n",
    "                    loc_span.next_sibling.text,\n",
    "                    ya_city(loc_span),\n",
    "                    ya_price(div)\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print('skipped a boat from: '+yard_str)\n",
    "    return l\n",
    "\n",
    "@none_on_error\n",
    "def ya_next_url(soup):\n",
    "    return 'https://www.yachtall.com'+soup.find('a', string='►')['href']\n",
    "\n",
    "def ya_listings(make):\n",
    "    url_template = 'https://www.yachtall.com/en/sailboats/used-boats?q={}&sprct={}'\n",
    "    next_url = url_template.format(make.replace(' ','%20'),max_price)\n",
    "    \n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        l += ya_scrape(make,soup)\n",
    "        next_url = ya_next_url(soup)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex_python.converter import CurrencyRates\n",
    "fx = CurrencyRates()\n",
    "eur_dkk = fx.get_rate('EUR', 'DKK')\n",
    "\n",
    "@none_on_error\n",
    "def dba_year(tr):\n",
    "    return int(tr.find('td',title='Modelår').text)\n",
    "\n",
    "def dba_scrape(make,soup):\n",
    "    listings = soup.find_all('tr', class_='dbaListing')\n",
    "    \n",
    "    l = list()\n",
    "    for tr in listings:\n",
    "        desc = tr.find_all('a', class_='listingLink')[1].text\n",
    "        if make in desc:\n",
    "            script_tag = tr.find('script',{'type':'application/ld+json'})\n",
    "            if script_tag:\n",
    "                j = json.loads(script_tag.contents[0])\n",
    "                l.append(\n",
    "                    (\n",
    "                        j['url'],\n",
    "                        j['name'].split(',')[0].replace(make,''),\n",
    "                        dba_year(tr),\n",
    "                        'Denmark',\n",
    "                        None,\n",
    "                        int(int(j['offers']['price'])/eur_dkk)\n",
    "                    )\n",
    "                )\n",
    "    return l        \n",
    "\n",
    "@none_on_error\n",
    "def dba_next_url(soup):\n",
    "    return 'https://www.dba.dk'+soup.find('a', string='Næste ')['href']\n",
    "\n",
    "def dba_listings(make):\n",
    "    url_template = 'https://www.dba.dk/baade/baade/sejlbaade/?soeg={}&pris=(-{})'\n",
    "    next_url = url_template.format(make.replace(' ','+'),max_price*eur_dkk)\n",
    "    \n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        l += dba_scrape(make,soup)\n",
    "        next_url = dba_next_url(soup)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_listings(make):\n",
    "    nv = nv_listings(make)\n",
    "    yw = yw_listings(make)\n",
    "    b24 = b24_listings(make)\n",
    "    tym = tym_listings(make)\n",
    "    sb = sb_listings(make)\n",
    "    bcom = bcom_listings(make)\n",
    "    ya = ya_listings(make)\n",
    "    dba = dba_listings(make)\n",
    "\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        nv+yw+b24+tym+sb+bcom+ya+dba,\n",
    "        columns=['url','model','year','country','city','price']\n",
    "    )\n",
    "\n",
    "    df.model = df.model.str.strip()\n",
    "    df.country = df.country.str.strip()\n",
    "    df.city = df.city.str.strip()\n",
    "\n",
    "\n",
    "    df = df[df.price <= max_price]\n",
    "    \n",
    "    df = df.round({\n",
    "        'year': 0,\n",
    "        'price': 0\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "if not debug: listings_make = memoize(scrape_listings)   \n",
    "else: listings_make = scrape_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "a4_landscape = (11.7, 8.27)\n",
    "a4_portrait = (8.27,11.7)\n",
    "    \n",
    "def scatter_year(df, \n",
    "                 ref=None, \n",
    "                 hline=100000):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=a4_landscape,facecolor='w')\n",
    "    ax = sns.scatterplot(\n",
    "        ax=ax, \n",
    "        data=df, \n",
    "        x='year', \n",
    "        y='price',\n",
    "        style=df.model.str.extract(r'\\D*([\\d\\.]*)')[0].tolist(), #https://github.com/mwaskom/seaborn/issues/2194\n",
    "        hue=df.country.tolist(), #https://github.com/mwaskom/seaborn/issues/2194\n",
    "#        size=df.loa.tolist(), #https://github.com/mwaskom/seaborn/issues/2194\n",
    "#        sizes=(200,400),\n",
    "        s=300,\n",
    "        legend='brief'\n",
    "    )\n",
    "    sns.regplot(\n",
    "        ax=ax, \n",
    "        data=df, \n",
    "        x='year', \n",
    "        y='price',\n",
    "        scatter=False\n",
    "    )\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "    if ref: ax.plot(ref[0], ref[1], 'rx', markersize=25)\n",
    "    if hline: ax.axhline(hline,ls='--',color='r')\n",
    "    return ax\n",
    "\n",
    "\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "def regplot(df,ax=None):\n",
    "    if not ax: fig, ax = plt.subplots(figsize=a4_landscape)    \n",
    "    warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "    return sns.regplot(ax = ax, x=\"year\", y=\"price\", data=df, robust=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def listings(make, model=None, model_excl=None, min_year=None, max_year=None):\n",
    "    df = listings_make(make)\n",
    "    \n",
    "    if min_year: df = df[df.year >= min_year]\n",
    "    if max_year: df = df[df.year <= max_year]\n",
    "        \n",
    "    if model: df = df[df.model.str.contains(model,case=False)]\n",
    "    if model_excl: df = df[~df.model.str.contains(model_excl,case=False)]\n",
    "\n",
    "        \n",
    "    return df.sort_values(by='price')\n",
    "\n",
    "def url_to_html_anchor(url):\n",
    "    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(url,url)\n",
    "\n",
    "def diplay_listings(df):\n",
    "    display(df.style.format(\n",
    "        {\n",
    "            'url': url_to_html_anchor,\n",
    "            'year': '{:n}',\n",
    "            'price': '{:n} €',\n",
    "        }\n",
    "    ))\n",
    "\n",
    "def summary(make, \n",
    "            model=None, \n",
    "            model_excl=None, \n",
    "            min_year=None, \n",
    "            max_year=None, \n",
    "            ref=None, \n",
    "            hline=None, \n",
    "            excl_ids=[]):\n",
    "    \n",
    "    title = make\n",
    "    if model: title += '_model({})'.format(model)\n",
    "    if min_year or max_year:\n",
    "        title += '_year[{},{}]'.format(min_year,max_year)\n",
    "    \n",
    "    df = listings(make, model, model_excl, min_year, max_year)\n",
    "    df.to_csv('out/listings-'+title+'.csv')\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['year','country','price'])\n",
    "    df = df.drop_duplicates(subset=['year','city'])\n",
    "    \n",
    "    df = df.drop(excl_ids)\n",
    "\n",
    "    ax = scatter_year(df,ref,hline)\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "    plt.savefig('out/'+title+'.pdf')\n",
    "    diplay_listings(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Queries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Finngulf',\n",
    "    model='3\\d'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Dehler',\n",
    "    max_year=2011,\n",
    "    model='3[6-8]',\n",
    "    ref=(2006,89000)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Dehler',\n",
    "    max_year=2012,\n",
    "    model='3[4|5]',\n",
    "    ref=(2008,104000)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Dehler',\n",
    "    model='3[4|5]',\n",
    "    ref=(2008,104000)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'X-Yachts',\n",
    "    model='3[3-8]2',\n",
    "    hline=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'X-Yachts',\n",
    "    model='4\\d2',\n",
    "    ref=(1999,126000)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'J Boats',\n",
    "    model='109',\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'J Boats',\n",
    "    model='1[0-1]\\d',\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'J Boats',\n",
    "    model='109'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Elan',\n",
    "    model='40',\n",
    "    model_excl='340',\n",
    "    ref=(2004,95000),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Elan',\n",
    "    model='37',\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Elan',\n",
    "    model='333',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\n",
    "    'Elan',\n",
    "    model='40',\n",
    "    ref=(2002,57500),\n",
    "    max_year=2008\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Grand Soleil',\n",
    "    model='45'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Arcona',\n",
    "    min_year=1980\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Sweden Yachts',\n",
    "    model='3[6-9]'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Salona',\n",
    "    model='37|38'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
