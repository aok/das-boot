{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_price=500000\n",
    "debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pickle\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pycountry\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Path('out').mkdir(parents=True, exist_ok=True)\n",
    "Path('cache').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "cache_filename = 'cache/page_cache-'+date.today().isoformat()+'.pickle'\n",
    "page_cache = {}\n",
    "try:\n",
    "    with open(cache_filename, 'rb') as f:\n",
    "        page_cache = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(cache_filename+' not found, starting fresh')\n",
    "    \n",
    "def save_obj(obj, name):\n",
    "    with open(cache_filename, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def memoize(f):\n",
    "    memo = {}\n",
    "    def helper(x):\n",
    "        if x not in memo:            \n",
    "            memo[x] = f(x)\n",
    "        return memo[x]\n",
    "    return helper\n",
    "\n",
    "def none_on_error(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            if debug: print(func.__name__+' FAILED with '+str(e))\n",
    "            return None\n",
    "    return wrapper\n",
    "\n",
    "def get_page(url):\n",
    "    if url not in page_cache:\n",
    "        r = requests.get(url, allow_redirects=False)\n",
    "        if r.status_code == 200:\n",
    "            page = r.text\n",
    "            page_cache[url] = page\n",
    "            return page\n",
    "        else:\n",
    "            print('GOT '+str(r.status_code)+' for GET '+url)\n",
    "            print(r.headers)\n",
    "            return ''\n",
    "    return page_cache[url]\n",
    "\n",
    "def make_soup(url):\n",
    "    return BeautifulSoup(\n",
    "        get_page(url),\n",
    "        'html.parser')\n",
    "\n",
    "if debug: get_soup = make_soup\n",
    "else: get_soup = memoize(make_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettivene.com\n",
    "\n",
    "re_loa = re.compile(r'(\\D*)([\\d\\.\\,]+)(\\D*)')\n",
    "\n",
    "@none_on_error\n",
    "def nv_price(s):\n",
    "    return float(s.replace(' ','').replace('â‚¬',''))\n",
    "\n",
    "@none_on_error\n",
    "def nv_loa(url):\n",
    "    soup = get_soup(url)\n",
    "    str_loa = soup.find('td',string='Length').next_sibling.next_sibling.text\n",
    "    return float(\n",
    "        re.match(\n",
    "            re_loa,\n",
    "            str_loa\n",
    "        ).group(2).replace(',','.')\n",
    "    )\n",
    "\n",
    "@none_on_error\n",
    "def nv_year(parent_div):\n",
    "    y_str = re.search(\n",
    "        r'(\\d\\d\\d\\d)',\n",
    "        parent_div.text\n",
    "    ).groups()[0]\n",
    "    return int(y_str)\n",
    "\n",
    "@none_on_error\n",
    "def nv_country(s):\n",
    "    s = s.split()[0]\n",
    "    if s in [c.name for c in pycountry.countries]: return s.strip()\n",
    "#    if s not in ('Helsinki','Espoo','Turku','Raisio'): print(s+' presumed to be in Finland')\n",
    "    return 'Finland'\n",
    "\n",
    "@none_on_error\n",
    "def nv_next_page_url(soup):\n",
    "    return soup.find(\n",
    "            'a',\n",
    "            class_='pageNavigation next_link'\n",
    "        )['href']\n",
    "\n",
    "def nv_parse_list_page(make,soup):    \n",
    "    urls = [\n",
    "        a['href'] \n",
    "        for a in soup.findAll('a',class_='childVifUrl')\n",
    "    ]\n",
    "    models = [\n",
    "        div.text.replace(make,'').strip()\n",
    "        for div in soup.findAll('div',class_='make_model_link')\n",
    "    ]\n",
    "    years = [\n",
    "        nv_year(div) \n",
    "        for div in soup.findAll('div',class_='vehicle_other_info clearfix_nett')\n",
    "    ]\n",
    "    lengths = [\n",
    "        nv_loa(url) \n",
    "        for url in urls\n",
    "    ]\n",
    "    locs = [\n",
    "        nv_country(div.b.text) \n",
    "        for div in soup.findAll('div',class_='location_info')\n",
    "    ]\n",
    "    prices = [\n",
    "        nv_price(div.text) \n",
    "        for div in soup.findAll('div',class_='main_price')\n",
    "    ]\n",
    "\n",
    "    return list(\n",
    "        zip(\n",
    "            urls,\n",
    "            models,\n",
    "            years,\n",
    "            lengths,\n",
    "            locs,\n",
    "            prices,\n",
    "        )\n",
    "    )\n",
    "\n",
    "def nv_listings(make):\n",
    "    next_url = 'https://www.nettivene.com/en/purjevene/'+make.replace(' ','-').lower()\n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        l += nv_parse_list_page(\n",
    "            make,\n",
    "            soup\n",
    "        )\n",
    "        next_url = nv_next_page_url(soup)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yachtworld\n",
    "\n",
    "@none_on_error\n",
    "def yw_redux_state_json(soup):\n",
    "    script_tag = soup.find('script',string=re.compile('__REDUX_STATE__')).contents[0]\n",
    "    json_str = script_tag[script_tag.index('window.__REDUX_STATE__ = ')+25:script_tag.rfind('}')+1]\n",
    "    return json.loads(json_str)    \n",
    "\n",
    "@none_on_error\n",
    "def yw_price(record):\n",
    "    return record['price']['type']['amount']['EUR']\n",
    "\n",
    "def yw_country(record):\n",
    "    cc = record['location']['countryCode']\n",
    "    country = pycountry.countries.get(alpha_2=cc)\n",
    "    if country: return country.name\n",
    "    return cc\n",
    "\n",
    "def yw_collect_listings(js):\n",
    "    records = js['search']['searchResults']['search']['records']\n",
    "    return [\n",
    "        (\n",
    "            r['mappedURL'],\n",
    "            r['model'],\n",
    "            r['year'],\n",
    "            r['boat']['specifications']['dimensions']['lengths']['nominal']['m'],\n",
    "            yw_country(r),\n",
    "            yw_price(r)\n",
    "        ) for r in records\n",
    "    ]\n",
    "\n",
    "def yw_has_next(js):\n",
    "    curr_page = int(js['search']['searchResults']['search']['currentPage'])\n",
    "    last_page = int(js['search']['searchResults']['search']['lastPage'])\n",
    "    return (curr_page<last_page)    \n",
    "\n",
    "\n",
    "def yw_listings(make):\n",
    "    url_template='https://www.yachtworld.com/boats-for-sale/condition-used/type-sail/make-{}/?currency=EUR&price=0-{}'\n",
    "    base_url = url_template.format(make,max_price)\n",
    "    url = base_url\n",
    "\n",
    "    l = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        js = yw_redux_state_json(get_soup(url))\n",
    "        if js:\n",
    "            l += yw_collect_listings(js)\n",
    "        \n",
    "            if yw_has_next(js):\n",
    "                page += 1\n",
    "                url = base_url+'&page='+str(page)\n",
    "            else: break\n",
    "        else: break\n",
    "        \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boat24\n",
    "\n",
    "@none_on_error\n",
    "def parse_b24_price(s):\n",
    "    return float(''.join(re.findall(r'\\d+', s)))\n",
    "    \n",
    "@none_on_error\n",
    "def parse_b24_loa(details_str):\n",
    "    loa_str = re.search(\n",
    "            r'([\\d\\.]+) x .*',\n",
    "            details_str\n",
    "        ).groups()[0]\n",
    "    return float(loa_str)\n",
    "\n",
    "@none_on_error\n",
    "def b24_country(s):\n",
    "    return s.split()[0]\n",
    "\n",
    "def b24_scrape(make,soup):    \n",
    "    \n",
    "    divs = soup.findAll('div', class_='bd')\n",
    "    \n",
    "    urls = [\n",
    "        div.h5.a['href']\n",
    "        for div in divs\n",
    "    ]\n",
    "    models = [\n",
    "        div.h5.a['title'].replace(make,'').strip()\n",
    "        for div in divs\n",
    "    ]\n",
    "    years = [\n",
    "        int(t.next_sibling) \n",
    "        for t in soup.findAll('label',string='Year Built')\n",
    "    ]\n",
    "    lengths = [\n",
    "        parse_b24_loa(div.text) \n",
    "        for div in soup.findAll('div',class_='details')\n",
    "    ]\n",
    "    locs = [\n",
    "        b24_country(div.text) \n",
    "        for div in soup.findAll('div',class_='location')\n",
    "    ]\n",
    "    prices = [\n",
    "        parse_b24_price(t.contents[0]) \n",
    "        for t in soup.findAll('p',class_='price')\n",
    "    ]\n",
    "    \n",
    "    return list(\n",
    "        zip(\n",
    "            urls,\n",
    "            models,\n",
    "            years,\n",
    "            lengths,\n",
    "            locs,\n",
    "            prices,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "@none_on_error\n",
    "def b24_next_url(soup):\n",
    "    return soup.find('a', class_='next')['href']\n",
    "\n",
    "def b24_listings(make):\n",
    "    next_url = 'https://www.boat24.com/en/sailboats/?src={}&mode=AND&whr=EUR&prs_min=&prs_max={}'.format(\n",
    "        make.replace(' ','+'),\n",
    "        max_price\n",
    "    )\n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        l += b24_scrape(make,soup)\n",
    "        next_url = b24_next_url(soup)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yachtmarket\n",
    "\n",
    "@none_on_error\n",
    "def loa_from(s):\n",
    "    return float(s.replace('m',''))\n",
    "\n",
    "@none_on_error\n",
    "def price_from(s):\n",
    "    return int(s.replace('â‚¬','').replace('EUR','').replace(',','').strip())\n",
    "\n",
    "\n",
    "def ym_scrape(make, soup):\n",
    "\n",
    "    anchors = soup.findAll('a',class_='boat-name')\n",
    "\n",
    "    urls = [\n",
    "        'https://www.theyachtmarket.com'+a['href'].split(sep='?')[0] for a in anchors\n",
    "    ]\n",
    "    \n",
    "    pattern = re.compile(make, re.IGNORECASE)\n",
    "    models = [pattern.sub('', a.text).strip() for a in anchors]\n",
    "\n",
    "    overviews = [div.text.split('|') for div in soup.findAll('div',class_='overview')]\n",
    "\n",
    "    years = [int(o[0]) for o in overviews]\n",
    "\n",
    "    lengths = [loa_from(o[1]) for o in overviews]\n",
    "\n",
    "    locs = [div.text.split(',')[-1].strip() for div in soup.findAll('div',class_='location')]\n",
    "\n",
    "    prices = [price_from(div.span.text) for div in soup.findAll('div',class_='pricing')]\n",
    "    \n",
    "    return list(\n",
    "        zip(\n",
    "            urls,\n",
    "            models,\n",
    "            years,\n",
    "            lengths,\n",
    "            locs,\n",
    "            prices,\n",
    "        )\n",
    "    )\n",
    "\n",
    "@none_on_error\n",
    "def ym_next_url(soup):\n",
    "    return 'https://www.theyachtmarket.com/en/boats-for-sale/search/'+soup.find('a', rel='next')['href']\n",
    "\n",
    "def ym_listings(make):\n",
    "    next_url = 'https://www.theyachtmarket.com/en/boats-for-sale/search/?manufacturermodel={}&currency=eur&lengthunit=metres&showsail=1'.format(\n",
    "            make.replace(' ','+').lower()\n",
    "        )\n",
    "    \n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        l += ym_scrape(make,soup)\n",
    "        next_url = ym_next_url(soup)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scanboat.com\n",
    "@none_on_error\n",
    "def sb_loa(url):\n",
    "    soup = get_soup(url)\n",
    "    l = soup.find('p',string='Length')\n",
    "    return float(l.next_sibling.next_sibling.text)\n",
    "\n",
    "@none_on_error\n",
    "def sb_price(s):\n",
    "    return int(s.p.text.replace('EUR','').replace(',','').strip())\n",
    "\n",
    "def sb_scrape(make, soup):\n",
    "    divs = soup.findAll('div', class_='item')\n",
    "    urls = ['https://www.scanboat.com'+div.a['href'] for div in divs]\n",
    "\n",
    "    sections = soup.findAll('section', class_='flex-1')\n",
    "    models = [s.h2.text.replace(make, '').replace(' - ','').strip() for s in sections]\n",
    "\n",
    "    bodies = soup.findAll('section', class_='item__body')\n",
    "\n",
    "    deets = [div.p.text.strip().split('|') for div in bodies]\n",
    "\n",
    "    years = [int(d[1].replace('Year :','').strip()) for d in deets]\n",
    "\n",
    "    locs = [d[2].replace('Country :','').strip() for d in deets]\n",
    "    \n",
    "    sections = soup.findAll('section', class_='flex-2 right')\n",
    "    prices = [sb_price(s) for s in sections]\n",
    "    \n",
    "    lengths = [sb_loa(u) for u in urls]\n",
    "\n",
    "    return list(\n",
    "        zip(\n",
    "            urls,\n",
    "            models,\n",
    "            years,\n",
    "            lengths,\n",
    "            locs,\n",
    "            prices,\n",
    "        )\n",
    "    )\n",
    "\n",
    "@none_on_error\n",
    "def sb_next_url(soup):\n",
    "    return 'https://www.scanboat.com'+soup.find('a',string='Next')['href']\n",
    "\n",
    "    \n",
    "def sb_listings(make):\n",
    "    next_url = 'https://www.scanboat.com/en/boats?SearchCriteria.BoatModelText={}&SearchCriteria.BoatTypeID=1&SearchCriteria.Searched=true&SearchCriteria.ExtendedSearch=False'.format(make.replace(' ','+').lower())\n",
    "    \n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        l += sb_scrape(make,soup)\n",
    "        next_url = sb_next_url(soup)\n",
    "    return l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_listings(make):\n",
    "    nv = nv_listings(make)\n",
    "    yw = yw_listings(make)\n",
    "    b24 = b24_listings(make)\n",
    "    ym = ym_listings(make)\n",
    "    sb = sb_listings(make)\n",
    "    \n",
    "    df = pd.DataFrame(nv+yw+b24+ym+sb,columns=['url','model','year','loa','location','price'])\n",
    "    df.to_csv('out/listings-'+make.replace(' ', '_').lower()+'.csv')\n",
    "    \n",
    "    df = df.round({\n",
    "        'year': 0,\n",
    "        'loa': 2,\n",
    "        'price': 0\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if debug: listings_make = scrape_listings\n",
    "else: listings_make = memoize(scrape_listings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex_python.converter import CurrencyRates\n",
    "fx = CurrencyRates()\n",
    "\n",
    "def sb_history(make,model):\n",
    "    make = make.lower().replace(' ','-')\n",
    "    model = model.lower().replace(' ','-')\n",
    "    url = 'https://www.sokbat.se/Modell/{}/{}'.format(make,model)\n",
    "    page = get_page(url)\n",
    "    item_id = re.search(r'CurentItemId = (\\d+);',page).group(1)\n",
    "    str_json = requests.post('https://www.sokbat.se/DataBase/GetPrices?itemId='+item_id).text\n",
    "    \n",
    "    df = pd.read_json(str_json[8:-1],orient='records')\n",
    "    \n",
    "    df['age'] = df.SalesYear.astype(int) - df.ItemYear.astype(int)\n",
    "    df['price_sek'] = df.SalesPrice.str.replace(re.compile(r'\\s'), '')\n",
    "    df['price_eur'] = df.price_sek.astype(float) * fx.get_rate('SEK', 'EUR')\n",
    "    \n",
    "    return sns.lmplot(x=\"age\", y=\"price_eur\", data=df[df.ItemYear > 0], robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_re_year_sold = re.compile(r'Sold: (\\d\\d\\d\\d-\\d\\d-\\d\\d)')\n",
    "@none_on_error\n",
    "def ba_date_sold(td):\n",
    "    return date.fromisoformat(re.search(ba_re_year_sold,td.font.text).group(1))\n",
    "\n",
    "ba_re_price = re.compile(r'(\\d[\\d\\s]+)\\sEUR')\n",
    "@none_on_error\n",
    "def ba_price(td):\n",
    "    return int(re.search(ba_re_price,td.p.text).group(1).replace(u'\\xa0', ''))\n",
    "\n",
    "def ba_listings(make):\n",
    "    soup = get_soup('http://www.boatagent.com/?sajt=kopbat_sokmotor&sokord='+make.lower().replace(' ','+'))\n",
    "    tds = soup.findAll('td', class_='batkatalog')\n",
    "    \n",
    "    urls = ['http://www.boatagent.com'+td.a['href'] for td in tds]\n",
    "    \n",
    "    models = [td.h2.text.replace(make,'') for td in tds]\n",
    "    \n",
    "    re_year = re.compile(r'Year of production: (\\d\\d\\d\\d)')\n",
    "    years = [re.search(re_year,td.p.text).group(1) for td in tds]\n",
    "    \n",
    "    dates_sold = [ba_date_sold(td) for td in tds]\n",
    "        \n",
    "    prices = [ba_price(td) for td in tds]\n",
    "    \n",
    "    df = pd.DataFrame(data=list(zip(urls,models,years,dates_sold,prices)),columns=['url','model','year','date_sold','price',])\n",
    "    \n",
    "    #df['age'] = df.year - df.date_sold.dt.year\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "a4_landscape = (11.7, 8.27)\n",
    "a4_portrait = (8.27,11.7)\n",
    "    \n",
    "def scatter(df,x,size,ax=None):\n",
    "    if not ax: fig, ax = plt.subplots(figsize=a4_landscape)\n",
    "    ax = sns.scatterplot(\n",
    "        ax=ax, \n",
    "        data=df, \n",
    "        x=x, \n",
    "        y='price',\n",
    "        size=df[size].tolist(), #https://github.com/mwaskom/seaborn/issues/2194\n",
    "        hue=df.location.tolist(), #https://github.com/mwaskom/seaborn/issues/2194\n",
    "#        sizes=(40, 400),\n",
    "        alpha=.5,\n",
    "        palette=\"muted\"\n",
    "    )\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "    return ax\n",
    "\n",
    "def scatter_year(df,ax=None):\n",
    "    return scatter(df,'year','loa',ax)\n",
    "\n",
    "def scatter_loa(df,ax=None):\n",
    "    return scatter(df,'loa','year',ax)\n",
    "\n",
    "\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "def regplot(df,ax=None):\n",
    "    if not ax: fig, ax = plt.subplots(figsize=a4_landscape)    \n",
    "    warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "    return sns.regplot(ax = ax, x=\"year\", y=\"price\", data=df, robust=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listings(make, model=None, min_year=None, max_year=None, min_loa=None, max_loa=None):\n",
    "    df = listings_make(make)\n",
    "    \n",
    "    if min_year: df = df[df.year >= min_year]\n",
    "    if max_year: df = df[df.year <= max_year]\n",
    "    if min_loa: df = df[df.loa >= min_loa]\n",
    "    if max_loa: df = df[df.loa <= max_loa]\n",
    "        \n",
    "    if model: df = df[df.model.str.contains(model,case=False)]\n",
    "        \n",
    "    return df.sort_values(by='price')\n",
    "\n",
    "def url_to_html_anchor(url):\n",
    "    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(url,url)\n",
    "\n",
    "def diplay_listings(df):\n",
    "    display(df.style.format(\n",
    "        {\n",
    "            'url': url_to_html_anchor,\n",
    "            'year': '{:n}',\n",
    "            'loa': '{:.2f} m',\n",
    "            'price': '{:n} â‚¬',\n",
    "        }\n",
    "    ))\n",
    "\n",
    "def summary(df):\n",
    "    display(\n",
    "        df.groupby('model').count()[['url']].rename(columns={'url':'count'})\n",
    "    )\n",
    "    fig, (ax1, ax2) = plt.subplots(2,1, figsize=a4_portrait)\n",
    "    scatter_year(df,ax=ax1)\n",
    "    scatter_loa(df,ax=ax2)\n",
    "    plt.show()\n",
    "    diplay_listings(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\n",
    "    listings(\n",
    "        'X-Yachts',\n",
    "        max_loa=13\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving page cache file\n",
    "save_obj(page_cache,cache_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
