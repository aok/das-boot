{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_price=500000\n",
    "debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pickle\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pycountry\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Path('out').mkdir(parents=True, exist_ok=True)\n",
    "Path('cache').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "cache_filename = 'cache/page_cache-'+date.today().isoformat()+'.pickle'\n",
    "page_cache = {}\n",
    "try:\n",
    "    with open(cache_filename, 'rb') as f:\n",
    "        page_cache = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(cache_filename+' not found, starting fresh')\n",
    "    \n",
    "def save_obj(obj, name):\n",
    "    with open(cache_filename, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def memoize(f):\n",
    "    memo = {}\n",
    "    def helper(x):\n",
    "        if x not in memo:            \n",
    "            memo[x] = f(x)\n",
    "        return memo[x]\n",
    "    return helper\n",
    "\n",
    "def none_on_error(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            if debug: print(func.__name__+' FAILED with '+str(e))\n",
    "            return None\n",
    "    return wrapper\n",
    "\n",
    "def get_page(url):\n",
    "    if url not in page_cache:\n",
    "        if debug: print('GET '+url)\n",
    "        r = requests.get(url, allow_redirects=False)\n",
    "        if r.status_code == 200:\n",
    "            page = r.text\n",
    "            page_cache[url] = page\n",
    "            return page\n",
    "        else:\n",
    "            print('GOT '+str(r.status_code)+' for GET '+url)\n",
    "            print(r.headers)\n",
    "            return ''\n",
    "    if debug: print('CACHE HIT '+url)\n",
    "    return page_cache[url]\n",
    "\n",
    "def make_soup(url):\n",
    "    return BeautifulSoup(\n",
    "        get_page(url),\n",
    "        'html.parser')\n",
    "\n",
    "if not debug: get_soup = memoize(make_soup)\n",
    "else: get_soup = make_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nettivene.com\n",
    "\n",
    "re_loa = re.compile(r'(\\D*)([\\d\\.\\,]+)(\\D*)')\n",
    "\n",
    "@none_on_error\n",
    "def nv_loa(url):\n",
    "    soup = get_soup(url)\n",
    "    str_loa = soup.find('td',string='Length').next_sibling.next_sibling.text\n",
    "    return float(\n",
    "        re_loa.match(\n",
    "            str_loa\n",
    "        ).group(2).replace(',','.')\n",
    "    )\n",
    "\n",
    "@none_on_error\n",
    "def nv_year(div):\n",
    "    sub_div = div.find('div',class_='vehicle_other_info')\n",
    "    return int(sub_div.ul.li.text)\n",
    "\n",
    "@none_on_error\n",
    "def nv_country_city(div):\n",
    "    sub_div = div.find('div',class_='location_info')\n",
    "    s = sub_div.b.text.split()[0].strip()\n",
    "    if s in [c.name for c in pycountry.countries]: return s, None\n",
    "    return 'Finland', s\n",
    "\n",
    "@none_on_error\n",
    "def nv_price(div):\n",
    "    sub_div = div.find('div',class_='main_price')\n",
    "    return int(sub_div.text.replace(' ','').replace('€',''))\n",
    "\n",
    "\n",
    "def nv_parse_list_page(make,soup):\n",
    "    divs = soup.findAll('div', class_='listingVifUrl')\n",
    "    l = []\n",
    "    for div in divs:\n",
    "        country, city = nv_country_city(div)\n",
    "        l.append(\n",
    "            (\n",
    "                div.div.a['href'],\n",
    "                div.div.a.text.replace(make,''),\n",
    "                nv_year(div),\n",
    "                nv_loa(div.div.a['href']),\n",
    "                country,\n",
    "                city,\n",
    "                nv_price(div)\n",
    "\n",
    "            )\n",
    "        )\n",
    "    return l\n",
    "\n",
    "@none_on_error\n",
    "def nv_next_page_url(soup):\n",
    "    return soup.find(\n",
    "            'a',\n",
    "            class_='pageNavigation next_link'\n",
    "        )['href']\n",
    "\n",
    "\n",
    "def nv_listings(make):\n",
    "    next_url = 'https://www.nettivene.com/en/purjevene/'+make.replace(' ','-').lower()\n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        msg = soup.find('div', id='msg')\n",
    "        if msg:\n",
    "            print(msg.text)\n",
    "            return l        \n",
    "        l += nv_parse_list_page(\n",
    "            make,\n",
    "            soup\n",
    "        )\n",
    "        next_url = nv_next_page_url(soup)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#yachtworld\n",
    "\n",
    "@none_on_error\n",
    "def yw_redux_state_json(soup):\n",
    "    script_tag = soup.find('script',string=re.compile('__REDUX_STATE__')).contents[0]\n",
    "    json_str = script_tag[script_tag.index('window.__REDUX_STATE__ = ')+25:script_tag.rfind('}')+1]\n",
    "    return json.loads(json_str)    \n",
    "\n",
    "@none_on_error\n",
    "def yw_price(record):\n",
    "    return record['price']['type']['amount']['EUR']\n",
    "\n",
    "def yw_country(record):\n",
    "    cc = record['location']['countryCode']\n",
    "    country = pycountry.countries.get(alpha_2=cc)\n",
    "    if country: return country.name\n",
    "    return cc\n",
    "\n",
    "@none_on_error\n",
    "def yw_parse_record(r):\n",
    "    return (\n",
    "        r['mappedURL'],\n",
    "        r['model'],\n",
    "        r['year'],\n",
    "        r['boat']['specifications']['dimensions']['lengths']['nominal']['m'],\n",
    "        yw_country(r),\n",
    "        str(r['location']['city']),\n",
    "        yw_price(r)\n",
    "    )\n",
    "\n",
    "def yw_collect_listings(js):\n",
    "    records = js['search']['searchResults']['search']['records']\n",
    "    return [yw_parse_record(r) for r in records]\n",
    "\n",
    "def yw_has_next(js):\n",
    "    curr_page = int(js['search']['searchResults']['search']['currentPage'])\n",
    "    last_page = int(js['search']['searchResults']['search']['lastPage'])\n",
    "    return (curr_page<last_page)    \n",
    "\n",
    "\n",
    "def yw_listings(make):\n",
    "    url_template='https://www.yachtworld.com/boats-for-sale/condition-used/type-sail/make-{}/?currency=EUR&price=0-{}'\n",
    "    base_url = url_template.format(make,max_price)\n",
    "    url = base_url\n",
    "\n",
    "    l = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        js = yw_redux_state_json(get_soup(url))\n",
    "        if js:\n",
    "            l += yw_collect_listings(js)\n",
    "        \n",
    "            if yw_has_next(js):\n",
    "                page += 1\n",
    "                url = base_url+'&page='+str(page)\n",
    "            else: break\n",
    "        else: break\n",
    "        \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#boat24\n",
    "\n",
    "@none_on_error\n",
    "def b24_year(div):\n",
    "    return int(div.find('label',string='Year Built').next_sibling)\n",
    "\n",
    "re_b24_loa = re.compile(r'([\\d\\.]+) x .*')\n",
    "@none_on_error\n",
    "def b24_loa(div):\n",
    "    loa_str = re_b24_loa.search(div.find('div',class_='details').text).group(1)\n",
    "    return float(loa_str)\n",
    "\n",
    "@none_on_error\n",
    "def b24_country_city(div):\n",
    "    ss = div.find('div',class_='location').text.split('»')\n",
    "    country = ss[0].split('(')[0]\n",
    "    return country, ss[-1]\n",
    "\n",
    "re_b24_price = re.compile(r'EUR ([\\d\\.]*),-')\n",
    "@none_on_error\n",
    "def b24_price(div):\n",
    "    s = re_b24_price.search(div.find('p',class_='price').text).group(1)\n",
    "    return int(s.replace('.',''))\n",
    "\n",
    "def b24_scrape(make,soup):    \n",
    "    \n",
    "    divs = soup.findAll('div', class_='resultViewEntry')\n",
    "\n",
    "    l = []\n",
    "    for div in divs:\n",
    "        country, city = b24_country_city(div)\n",
    "        l.append(\n",
    "            (\n",
    "                div.div.a['href'],\n",
    "                div.div.a['title'].replace(make,''),\n",
    "                b24_year(div),\n",
    "                b24_loa(div),\n",
    "                country,\n",
    "                city,\n",
    "                b24_price(div),\n",
    "            )\n",
    "        )\n",
    "    return l\n",
    "    \n",
    "@none_on_error\n",
    "def b24_next_url(soup):\n",
    "    return soup.find('a', class_='next')['href']\n",
    "\n",
    "def b24_listings(make):\n",
    "    next_url = 'https://www.boat24.com/en/sailboats/?src={}&mode=AND&whr=EUR&prs_min=&prs_max={}'.format(\n",
    "        make.replace(' ','+'),\n",
    "        max_price\n",
    "    )\n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        l += b24_scrape(make,soup)\n",
    "        next_url = b24_next_url(soup)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#yachtmarket\n",
    "\n",
    "re_tym_loa = re.compile(r'\\s([\\d\\.]*)[Mm]')\n",
    "\n",
    "@none_on_error\n",
    "def tym_loa(ov):\n",
    "    return float(re_tym_loa.search(ov.text).group(1))\n",
    "\n",
    "re_tym_year = re.compile(r'(\\d\\d\\d\\d)\\D')\n",
    "\n",
    "@none_on_error\n",
    "def tym_year(ov):\n",
    "    return int(re_tym_year.search(ov.text).group(1))\n",
    "\n",
    "\n",
    "@none_on_error\n",
    "def tym_price(div):\n",
    "    return int(div.find('div',class_='pricing').span.text.replace('€','').replace('EUR','').replace(',',''))\n",
    "\n",
    "\n",
    "def tym_scrape(make, soup):\n",
    "\n",
    "    divs = soup.findAll('div',class_='result')\n",
    "\n",
    "    l = []\n",
    "    for div in divs:\n",
    "        a = div.find('a', class_='boat-name')\n",
    "        ov = div.find('div', class_='overview')\n",
    "        loc_a = div.find('div', class_='location').text.split(',')\n",
    "        l.append(\n",
    "            (\n",
    "                'https://www.theyachtmarket.com'+a['href'].split('?')[0],\n",
    "                a.text.replace(make,''),\n",
    "                tym_year(ov),\n",
    "                tym_loa(ov),\n",
    "                loc_a[-1],\n",
    "                loc_a[0],\n",
    "                tym_price(div)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    return l\n",
    "\n",
    "\n",
    "@none_on_error\n",
    "def tym_next_url(soup):\n",
    "    return 'https://www.theyachtmarket.com/en/boats-for-sale/search/'+soup.find('a', rel='next')['href']\n",
    "\n",
    "def tym_listings(make):\n",
    "    next_url = 'https://www.theyachtmarket.com/en/boats-for-sale/search/?manufacturermodel={}&currency=eur&lengthunit=metres&showsail=1'.format(\n",
    "            make.replace(' ','+').lower()\n",
    "        )\n",
    "    \n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        l += tym_scrape(make,soup)\n",
    "        next_url = tym_next_url(soup)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scanboat.com\n",
    "@none_on_error\n",
    "def sb_loa(url):\n",
    "    soup = get_soup(url)\n",
    "    l = soup.find('p',string='Length')\n",
    "    return float(l.next_sibling.next_sibling.text)\n",
    "\n",
    "@none_on_error\n",
    "def sb_price(s):\n",
    "    return int(s.p.text.replace('EUR','').replace(',',''))\n",
    "\n",
    "re_sb_year = re.compile(r'Year : (\\d\\d\\d\\d)')\n",
    "re_sb_country= re.compile(r'Country : (.*)')\n",
    "\n",
    "def sb_scrape(make, soup):\n",
    "    \n",
    "    divs = soup.findAll('div', class_='item')\n",
    "\n",
    "    l = []\n",
    "    for div in divs:\n",
    "        header = div.find('header',class_='item__header')\n",
    "        body = div.find('section',class_='item__body')\n",
    "        url = 'https://www.scanboat.com'+div.a['href']\n",
    "        price_tags = header.findAll('p')\n",
    "\n",
    "        if price_tags:\n",
    "            l.append(\n",
    "                (\n",
    "                    url,\n",
    "                    header.section.text.replace(make,'').replace(' - ',''),\n",
    "                    int(re_sb_year.search(body.p.text).group(1)),\n",
    "                    sb_loa(url),\n",
    "                    re_sb_country.search(body.p.text).group(1),\n",
    "                    None,\n",
    "                    int(price_tags[-1].text.replace('EUR','').replace(',','')),\n",
    "                )\n",
    "            )\n",
    "    return l    \n",
    "\n",
    "@none_on_error\n",
    "def sb_next_url(soup):\n",
    "    return 'https://www.scanboat.com'+soup.find('a',string='Next')['href']\n",
    "\n",
    "    \n",
    "def sb_listings(make):\n",
    "    next_url = 'https://www.scanboat.com/en/boats?SearchCriteria.BoatModelText={}&SearchCriteria.BoatTypeID=1&SearchCriteria.Searched=true&SearchCriteria.ExtendedSearch=False'.format(make.replace(' ','+').lower())\n",
    "    \n",
    "    l = []\n",
    "    while next_url:\n",
    "        soup = get_soup(next_url)\n",
    "        l += sb_scrape(make,soup)\n",
    "        next_url = sb_next_url(soup)\n",
    "    return l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_listings(make):\n",
    "    nv = nv_listings(make)\n",
    "    yw = yw_listings(make)\n",
    "    b24 = b24_listings(make)\n",
    "    tym = tym_listings(make)\n",
    "    sb = sb_listings(make)\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        nv+yw+b24+tym+sb,\n",
    "        columns=['url','model','year','loa','country','city','price']\n",
    "    )\n",
    "\n",
    "    df.model = df.model.str.strip()\n",
    "    df.country = df.country.str.strip()\n",
    "    df.city = df.city.str.strip()\n",
    "\n",
    "\n",
    "    df = df[df.price <= max_price]\n",
    "    \n",
    "    df = df.round({\n",
    "        'year': 0,\n",
    "        'loa': 2,\n",
    "        'price': 0\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "if not debug: listings_make = memoize(scrape_listings)   \n",
    "else: listings_make = scrape_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from forex_python.converter import CurrencyRates\n",
    "fx = CurrencyRates()\n",
    "\n",
    "def sokbat_history(make,model):\n",
    "    make = make.lower().replace(' ','-')\n",
    "    model = model.lower().replace(' ','-')\n",
    "    url = 'https://www.sokbat.se/Modell/{}/{}'.format(make,model)\n",
    "    page = get_page(url)\n",
    "    item_id = re.search(r'CurentItemId = (\\d+);',page).group(1)\n",
    "    str_json = requests.post('https://www.sokbat.se/DataBase/GetPrices?itemId='+item_id).text\n",
    "    \n",
    "    df = pd.read_json(str_json[8:-1],orient='records')\n",
    "    \n",
    "    df['age'] = df.SalesYear.astype(int) - df.ItemYear.astype(int)\n",
    "    df['price_sek'] = df.SalesPrice.str.replace(re.compile(r'\\s'), '')\n",
    "    df['price_eur'] = df.price_sek.astype(float) * fx.get_rate('SEK', 'EUR')\n",
    "    \n",
    "    return sns.lmplot(x=\"age\", y=\"price_eur\", data=df[df.ItemYear > 0], robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ba_re_year_sold = re.compile(r'Sold: (\\d\\d\\d\\d-\\d\\d-\\d\\d)')\n",
    "@none_on_error\n",
    "def ba_date_sold(td):\n",
    "    return date.fromisoformat(re.search(ba_re_year_sold,td.font.text).group(1))\n",
    "\n",
    "ba_re_price = re.compile(r'(\\d[\\d\\s]+)\\sEUR')\n",
    "@none_on_error\n",
    "def ba_price(td):\n",
    "    return int(re.search(ba_re_price,td.p.text).group(1).replace(u'\\xa0', ''))\n",
    "\n",
    "def ba_listings(make):\n",
    "    soup = get_soup('http://www.boatagent.com/?sajt=kopbat_sokmotor&sokord='+make.lower().replace(' ','+'))\n",
    "    tds = soup.findAll('td', class_='batkatalog')\n",
    "    \n",
    "    urls = ['http://www.boatagent.com'+td.a['href'] for td in tds]\n",
    "    \n",
    "    models = [td.h2.text.replace(make,'') for td in tds]\n",
    "    \n",
    "    re_year = re.compile(r'Year of production: (\\d\\d\\d\\d)')\n",
    "    years = [re.search(re_year,td.p.text).group(1) for td in tds]\n",
    "    \n",
    "    dates_sold = [ba_date_sold(td) for td in tds]\n",
    "        \n",
    "    prices = [ba_price(td) for td in tds]\n",
    "    \n",
    "    df = pd.DataFrame(data=list(zip(urls,models,years,dates_sold,prices)),columns=['url','model','year','date_sold','price',])\n",
    "    \n",
    "    #df['age'] = df.year - df.date_sold.dt.year\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "a4_landscape = (11.7, 8.27)\n",
    "a4_portrait = (8.27,11.7)\n",
    "    \n",
    "def scatter_year(df):\n",
    "    fig, ax = plt.subplots(figsize=a4_landscape,facecolor='w')\n",
    "    ax = sns.scatterplot(\n",
    "        ax=ax, \n",
    "        data=df, \n",
    "        x='year', \n",
    "        y='price',\n",
    "        style=df.model.str.extract(r'\\D*([\\d\\.]*)')[0].tolist(), #https://github.com/mwaskom/seaborn/issues/2194\n",
    "        hue=df.country.tolist(), #https://github.com/mwaskom/seaborn/issues/2194\n",
    "#        size=df.loa.tolist(), #https://github.com/mwaskom/seaborn/issues/2194\n",
    "#        sizes=(200,400),\n",
    "        s=300,\n",
    "        legend='brief'\n",
    "    )\n",
    "    sns.regplot(\n",
    "        ax=ax, \n",
    "        data=df, \n",
    "        x='year', \n",
    "        y='price',\n",
    "        scatter=False\n",
    "    )\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "    ax.axhline(100000,ls='--',color='r')\n",
    "    return ax\n",
    "\n",
    "\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "def regplot(df,ax=None):\n",
    "    if not ax: fig, ax = plt.subplots(figsize=a4_landscape)    \n",
    "    warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "    return sns.regplot(ax = ax, x=\"year\", y=\"price\", data=df, robust=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def listings(make, model=None, min_year=None, max_year=None, min_loa=None, max_loa=None):\n",
    "    df = listings_make(make)\n",
    "    \n",
    "    if min_year: df = df[df.year >= min_year]\n",
    "    if max_year: df = df[df.year <= max_year]\n",
    "    if min_loa: df = df[df.loa >= min_loa]\n",
    "    if max_loa: df = df[df.loa <= max_loa]\n",
    "        \n",
    "    if model: df = df[df.model.str.contains(model,case=False)]\n",
    "        \n",
    "    return df.sort_values(by='price')\n",
    "\n",
    "def url_to_html_anchor(url):\n",
    "    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(url,url)\n",
    "\n",
    "def diplay_listings(df):\n",
    "    display(df.style.format(\n",
    "        {\n",
    "            'url': url_to_html_anchor,\n",
    "            'year': '{:n}',\n",
    "            'loa': '{:.2f} m',\n",
    "            'price': '{:n} €',\n",
    "        }\n",
    "    ))\n",
    "\n",
    "def summary(make, model=None, min_year=None, max_year=None, min_loa=None, max_loa=None, ref=None):\n",
    "    title = make\n",
    "    if model: title += '_model({})'.format(model)\n",
    "    if min_year or max_year:\n",
    "        title += '_year[{},{}]'.format(min_year,max_year)\n",
    "        \n",
    "    if min_loa or max_loa:\n",
    "        title += '_loa[{},{}]'.format(min_loa,max_loa)\n",
    "    \n",
    "    df = listings(make, model, min_year, max_year, min_loa, max_loa)\n",
    "    df.to_csv('out/listings-'+title+'.csv')\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['year','country','price'])\n",
    "    df = df.drop_duplicates(subset=['year','city'])\n",
    "\n",
    "\n",
    "    ax = scatter_year(df)\n",
    "    if ref: ax.plot(ref[0], ref[1], 'rx', markersize=25)\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "    plt.savefig('out/'+title+'.pdf')\n",
    "    diplay_listings(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Finngulf',\n",
    "    max_loa=12\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Dehler',\n",
    "    max_year=2012,\n",
    "    model='3[3-6]'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Dehler',\n",
    "    min_year=2008,\n",
    "    max_year=2012,\n",
    "    model='35'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'X-Yachts',\n",
    "    model='40',\n",
    "    min_year=2004,\n",
    "    ref=(2006,149000)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'X-Yachts',\n",
    "    model='332'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'J Boats',\n",
    "    min_loa=9.5,\n",
    "    max_loa=11.1\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Elan',\n",
    "    model='40',\n",
    "    max_year=2006\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Grand Soleil',\n",
    "    model='37'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Arcona',\n",
    "    min_year=1980\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(\n",
    "    'Sweden Yachts',\n",
    "    model='3\\d'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\n",
    "    'Salona',\n",
    "    model='37|38'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saving page cache file\n",
    "save_obj(page_cache,cache_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
